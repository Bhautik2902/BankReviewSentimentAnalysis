{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4bFWX-mwHSs",
        "outputId": "a5b23f50-94ae-44d5-c21b-d0103b7f6228"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QuGDEedwPiY",
        "outputId": "fdc5fd67-942f-47d6-af4b-cbd4c97817c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ACC_project\t\t\t\t      LOR_CSE_HOD.docx\n",
            "'Admission Docs'\t\t\t      LOR_CSE_PROFESSOR.docx\n",
            " Admission_updated_docs\t\t\t      LOR_Manager_Naveen.docx\n",
            " ASE\t\t\t\t\t     'Masters Resources'\n",
            " ASP-Assignment-01\t\t\t     'Pre Entry - Interview Planning.xlsx'\n",
            " ASP-Assignment-02\t\t\t     'Project Final Report.gdoc'\n",
            " ASP-Assignment-03\t\t\t      Resume_Subhram_Satyajeet.docx\n",
            " ASP-Assignment-04\t\t\t     'Satyajeet Subhram -LOR by Prof Debahuti Mishra.pdf'\n",
            "'CAmbridge book 1-20220401T111740Z-001.zip'  'Satyajeet Subhram -LOR - Prof S P Pati.pdf'\n",
            "'CAmbridge book 1-20220401T111749Z-001.zip'   SOP_Subhram.docx\n",
            "'Cambridge book 2-20220401T111757Z-001.zip'   Subhram_Satyajeet_resume_Teksystems.pdf\n",
            " classifier.joblib\t\t\t     'TEKSYSTEMS Docs'\n",
            "'Colab Notebooks'\t\t\t     'Teksystems farewell'\n",
            " Course-Material.zip\t\t\t     'Topics in AI.zip'\n",
            " flow_diagram.drawio\t\t\t      video1296183071.mp4\n",
            "'IELTS books'\t\t\t\t      Visa_docs\n",
            " InternshipProject2\n"
          ]
        }
      ],
      "source": [
        "!ls /content/gdrive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eF1NZIfAw5r0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lPDpab5w6f_",
        "outputId": "71c1d2ea-707c-4eec-ea60-43757068f109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vPNVb6N_w2FF"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oKBMYhUPwx0Q"
      },
      "outputs": [],
      "source": [
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character accepted by lemmatizer\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UJL_8MjDwuWE"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Remove stopwords and non-alphabetic tokens\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    # Lemmatize tokens\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "#reviews_tokenized = reviews.apply(preprocess_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAOjsWh2wQNi",
        "outputId": "d977925a-fc84-4426-83ea-071b5d490239"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raw_canadian_banks_reddit_posts__credit_unions_2024_11_09-23_31_48\n",
            "Downloaded latest blob: gs://text-mining-source-dump/raw_canadian_banks_reddit_posts__credit_unions_2024_11_09-23_31_48 to unlabeled_raw_data.csv\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import storage\n",
        "from google.oauth2 import service_account\n",
        "import os\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "def download_latest_blob_with_timestamp(bucket_name):\n",
        "    \"\"\"Downloads the latest blob from the bucket and uses its timestamp as the filename.\"\"\"\n",
        "    SERVICE_ACCOUNT_FILE = 'amplified-brook-416922-6a39d3e05104.json'\n",
        "    credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE)\n",
        "\n",
        "    storage_client = storage.Client(project='amplified-brook-416922', credentials=credentials)\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # Get all blobs and then sort them by time_created\n",
        "    blobs = list(bucket.list_blobs())\n",
        "    blobs.sort(key=lambda blob: blob.time_created, reverse=True)\n",
        "    # Sort in descending order (newest first)\n",
        "\n",
        "    latest_blob = blobs[0] if blobs else None\n",
        "    print(latest_blob.name)\n",
        "    if latest_blob:\n",
        "        # Get the timestamp from the blob's metadata\n",
        "        timestamp = latest_blob.time_created.strftime(\"%Y%m%d_%H%M%S\")\n",
        "        # Format the timestamp as desired (e.g., YYYYMMDD_HHMMSS)\n",
        "\n",
        "        # Create the destination filename using the timestamp\n",
        "        #destination_file_name = os.path.join(destination_folder, f\"downloaded_file_{timestamp}.csv\")\n",
        "        destination_file_name = f\"unlabeled_raw_data.csv\"\n",
        "\n",
        "        # Download the latest blob to the specified folder\n",
        "        latest_blob.download_to_filename(destination_file_name)\n",
        "        print(f\"Downloaded latest blob: gs://{bucket_name}/{latest_blob.name} to {destination_file_name}\")\n",
        "    else:\n",
        "        print(\"No blobs found in the bucket.\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "bucket_name = 'text-mining-source-dump'  # Replace with your bucket name\n",
        "#destination_folder = '/path/to/your/destination/folder'  # Replace with your desired folder path\n",
        "\n",
        "download_latest_blob_with_timestamp(bucket_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbEjmCNbwZIV"
      },
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "from google.oauth2 import service_account\n",
        "import os\n",
        "\n",
        "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
        "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
        "    SERVICE_ACCOUNT_FILE = 'amplified-brook-416922-6a39d3e05104.json'  # Replace if needed\n",
        "    credentials = service_account.Credentials.from_service_account_file(\n",
        "        SERVICE_ACCOUNT_FILE\n",
        "    )\n",
        "\n",
        "    storage_client = storage.Client(project='amplified-brook-416922', credentials=credentials)  # Replace project ID if needed\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(source_blob_name)\n",
        "\n",
        "    blob.download_to_filename(destination_file_name)\n",
        "\n",
        "    print(\n",
        "        f\"Blob {source_blob_name} downloaded to {destination_file_name}.\"\n",
        "    )\n",
        "\n",
        "# Example usage:\n",
        "bucket_name = \"text-mining-source-dump\"  # Replace with your bucket name\n",
        "source_blob_name = \"aggregated_data.csv\"  # Replace with the name of the file in the bucket\n",
        "destination_file_name = \"unlabeled_raw_data.csv\"  # Replace with the desired local filename\n",
        "\n",
        "download_blob(bucket_name, source_blob_name, destination_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NdRktHwQitNa"
      },
      "outputs": [],
      "source": [
        "def get_review_vector(review, model):\n",
        "    review_vec = np.zeros(100)  # 100 is the vector size used in Word2Vec\n",
        "    count = 0\n",
        "    for word in review:\n",
        "        if word in model.wv.key_to_index:  # check if word is in the vocabulary\n",
        "            review_vec += model.wv[word]\n",
        "            count += 1\n",
        "    return review_vec / count if count > 0 else review_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1N0ewL47n2rt"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=3000)  # Use top 3000 words for features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LyZgvNLwcMH",
        "outputId": "373ba1a2-f1ec-4ab5-e15b-a0fcd1346126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted labels saved to labeled_reviews1.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# ... (Your existing functions: preprocess_text, get_review_vector) ...\n",
        "\n",
        "# Load the saved SVM model\n",
        "model_save_name = 'classifier.joblib'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
        "svm_classifier = joblib.load(path)\n",
        "\n",
        "# Load the new dataset\n",
        "new_data = pd.read_csv('unlabeled_raw_data.csv')\n",
        "new_reviews = new_data['review_text']\n",
        "\n",
        "label_mapping = {1: 'positive', -1: 'negative', 0: 'neutral'}\n",
        "\n",
        "# Preprocess the new reviews\n",
        "new_reviews_tokenized = new_reviews.apply(preprocess_text)\n",
        "\n",
        "word2vec_model = Word2Vec(sentences=new_reviews_tokenized, vector_size=100, window=5, min_count=2, sg=1)\n",
        "\n",
        "\n",
        "# Convert to Word2Vec vectors\n",
        "new_X_word2vec = np.array([get_review_vector(review, word2vec_model) for review in new_reviews_tokenized])\n",
        "\n",
        "# Convert to TF-IDF vectors (assuming tfidf_vectorizer is already fitted)\n",
        "new_reviews_joined = [' '.join(review) for review in new_reviews_tokenized]\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(new_reviews_joined).toarray()\n",
        "X_combined = np.hstack((new_X_word2vec, X_tfidf))\n",
        "new_X_tfidf = tfidf_vectorizer.transform(new_reviews_joined).toarray()\n",
        "# Combine Word2Vec and TF-IDF features\n",
        "new_X_combined = np.hstack((new_X_word2vec, new_X_tfidf))\n",
        "\n",
        "# Predict the sentiment for each review using the loaded model\n",
        "new_predictions = svm_classifier.predict(new_X_combined)\n",
        "new_predictions_text = [label_mapping[pred] for pred in new_predictions]\n",
        "\n",
        "# Print or store the predictions as needed\n",
        "# for i, prediction in enumerate(new_predictions_text):\n",
        "#     print(f\"Review {i + 1}: {prediction}\")\n",
        "\n",
        "# Save the predictions in the original DataFrame\n",
        "new_data['predicted_sentiment'] = new_predictions_text\n",
        "\n",
        "# Save to a new CSV file\n",
        "new_data.to_csv('labeled_review1.csv', index=False)\n",
        "\n",
        "print(\"Predicted labels saved to labeled_reviews1.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24MtDEtGwfh3"
      },
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "from google.oauth2 import service_account\n",
        "import os\n",
        "\n",
        "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
        "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
        "    SERVICE_ACCOUNT_FILE = 'amplified-brook-416922-6a39d3e05104.json'  # Replace if needed\n",
        "    credentials = service_account.Credentials.from_service_account_file(\n",
        "        SERVICE_ACCOUNT_FILE\n",
        "    )\n",
        "\n",
        "    storage_client = storage.Client(project='amplified-brook-416922', credentials=credentials)  # Replace project ID if needed\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(source_blob_name)\n",
        "\n",
        "    blob.download_to_filename(destination_file_name)\n",
        "\n",
        "    print(\n",
        "        f\"Blob {source_blob_name} downloaded to {destination_file_name}.\"\n",
        "    )\n",
        "\n",
        "# Example usage:\n",
        "bucket_name = \"text-mining-labeled-data\"  # Replace with your bucket name\n",
        "source_blob_name = \"labeled_reviews1\"  # Replace with the name of the file in the bucket\n",
        "destination_file_name = \"labeled_reviews.csv\"  # Replace with the desired local filename\n",
        "\n",
        "download_blob(bucket_name, source_blob_name, destination_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTyyOOWPwiiS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def concatenate_csv(file1, file2, output_file):\n",
        "    \"\"\"Concatenates two CSV files into a single CSV file.\n",
        "\n",
        "    Args:\n",
        "        file1 (str): Path to the first CSV file.\n",
        "        file2 (str): Path to the second CSV file.\n",
        "        output_file (str): Path to the output CSV file.\n",
        "    \"\"\"\n",
        "\n",
        "    # Read the CSV files into Pandas DataFrames\n",
        "    df1 = pd.read_csv(file1)\n",
        "    df2 = pd.read_csv(file2)\n",
        "\n",
        "    # Concatenate the DataFrames\n",
        "    concatenated_df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "    # Save the concatenated DataFrame to a new CSV file\n",
        "    concatenated_df.to_csv(output_file, index=False)\n",
        "\n",
        "    print(f\"CSV files concatenated and saved to {output_file}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "file1 = 'final_labeled_reviews.csv'\n",
        "file2 = 'labeled_review1.csv'\n",
        "output_file = 'concatenated_file.csv'\n",
        "\n",
        "concatenate_csv(file1, file2, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzOhDiiFwmH1"
      },
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "from google.oauth2 import service_account\n",
        "import os\n",
        "import csv\n",
        "\n",
        "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
        "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
        "    # The ID of your GCS bucket\n",
        "    #bucket_name = \"text-mining-source-dump\"\n",
        "    # The path to your file to upload\n",
        "    #source_file_name = \"C:\\Users\\Subhram Satyajeet\\OneDrive - University of Windsor\\Desktop\\Internship Project 2\\Review_Dataset_google_apple_net_banking\\Dataset\\bmo_google_before_2016.csv\"\n",
        "    # The ID of your GCS object\n",
        "    #destination_blob_name = \"bmo_google_before_2016\"\n",
        "\n",
        "    #Setting the service account credentials for authentication\n",
        "    SERVICE_ACCOUNT_FILE = 'amplified-brook-416922-6a39d3e05104.json' #key file name\n",
        "    credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE) #setting credentials using key file\n",
        "\n",
        "    storage_client = storage.Client(project='amplified-brook-416922' ,credentials = credentials)\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "    # Optional: set a generation-match precondition to avoid potential race conditions\n",
        "    # and data corruptions. The request to upload is aborted if the object's\n",
        "    # generation number does not match your precondition. For a destination\n",
        "    # object that does not yet exist, set the if_generation_match precondition to 0.\n",
        "    # If the destination object already exists in your bucket, set instead a\n",
        "    # generation-match precondition using its generation number.\n",
        "    generation_match_precondition = 0\n",
        "\n",
        "    blob.upload_from_filename(source_file_name, if_generation_match=generation_match_precondition)\n",
        "\n",
        "    print(\n",
        "        f\"File {source_file_name} uploaded to {destination_blob_name}.\"\n",
        "    )\n",
        "\n",
        "upload_blob('text-mining-labeled-data','concatenated_file.csv','final_labeled_reviews')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}