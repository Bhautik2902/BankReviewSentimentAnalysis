{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4bFWX-mwHSs",
        "outputId": "98b48fe7-f949-4cf8-9b86-641c460bb0c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QuGDEedwPiY",
        "outputId": "930c8a97-302a-4f79-b711-5739b52d80d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ACC_project\t\t\t\t      LOR_CSE_HOD.docx\n",
            "'Admission Docs'\t\t\t      LOR_CSE_PROFESSOR.docx\n",
            " Admission_updated_docs\t\t\t      LOR_Manager_Naveen.docx\n",
            " ASE\t\t\t\t\t     'Masters Resources'\n",
            " ASP-Assignment-01\t\t\t     'Pre Entry - Interview Planning.xlsx'\n",
            " ASP-Assignment-02\t\t\t     'Project Final Report.gdoc'\n",
            " ASP-Assignment-03\t\t\t      Resume_Subhram_Satyajeet.docx\n",
            " ASP-Assignment-04\t\t\t     'Satyajeet Subhram -LOR by Prof Debahuti Mishra.pdf'\n",
            "'CAmbridge book 1-20220401T111740Z-001.zip'  'Satyajeet Subhram -LOR - Prof S P Pati.pdf'\n",
            "'CAmbridge book 1-20220401T111749Z-001.zip'   SOP_Subhram.docx\n",
            "'Cambridge book 2-20220401T111757Z-001.zip'   Subhram_Satyajeet_resume_Teksystems.pdf\n",
            " classifier.joblib\t\t\t     'TEKSYSTEMS Docs'\n",
            "'Colab Notebooks'\t\t\t     'Teksystems farewell'\n",
            " Course-Material.zip\t\t\t     'Topics in AI.zip'\n",
            " flow_diagram.drawio\t\t\t      video1296183071.mp4\n",
            "'IELTS books'\t\t\t\t      Visa_docs\n",
            " InternshipProject2\n"
          ]
        }
      ],
      "source": [
        "!ls /content/gdrive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eF1NZIfAw5r0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lPDpab5w6f_",
        "outputId": "886ed643-eb19-4c83-ece4-2cbf7b0e51f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPNVb6N_w2FF"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKBMYhUPwx0Q"
      },
      "outputs": [],
      "source": [
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character accepted by lemmatizer\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJL_8MjDwuWE"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Remove stopwords and non-alphabetic tokens\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    # Lemmatize tokens\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "#reviews_tokenized = reviews.apply(preprocess_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAOjsWh2wQNi",
        "outputId": "75f3a9a6-6d65-4037-ed28-0c4dd0b57c88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raw_canadian_banks_reddit_posts__credit_unions_2024_11_09-23_31_48\n",
            "Downloaded latest blob: gs://text-mining-source-dump/raw_canadian_banks_reddit_posts__credit_unions_2024_11_09-23_31_48 to unlabeled_raw_data.csv\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import storage\n",
        "from google.oauth2 import service_account\n",
        "import os\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "def download_latest_blob_with_timestamp(bucket_name):\n",
        "    \"\"\"Downloads the latest blob from the bucket and uses its timestamp as the filename.\"\"\"\n",
        "    SERVICE_ACCOUNT_FILE = 'amplified-brook-416922-6a39d3e05104.json'\n",
        "    credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE)\n",
        "\n",
        "    storage_client = storage.Client(project='amplified-brook-416922', credentials=credentials)\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # Get all blobs and then sort them by time_created\n",
        "    blobs = list(bucket.list_blobs())\n",
        "    blobs.sort(key=lambda blob: blob.time_created, reverse=True)\n",
        "    # Sort in descending order (newest first)\n",
        "\n",
        "    latest_blob = blobs[0] if blobs else None\n",
        "    print(latest_blob.name)\n",
        "    if latest_blob:\n",
        "        # Get the timestamp from the blob's metadata\n",
        "        timestamp = latest_blob.time_created.strftime(\"%Y%m%d_%H%M%S\")\n",
        "        # Format the timestamp as desired (e.g., YYYYMMDD_HHMMSS)\n",
        "\n",
        "        # Create the destination filename using the timestamp\n",
        "        #destination_file_name = os.path.join(destination_folder, f\"downloaded_file_{timestamp}.csv\")\n",
        "        destination_file_name = f\"unlabeled_raw_data.csv\"\n",
        "\n",
        "        # Download the latest blob to the specified folder\n",
        "        latest_blob.download_to_filename(destination_file_name)\n",
        "        print(f\"Downloaded latest blob: gs://{bucket_name}/{latest_blob.name} to {destination_file_name}\")\n",
        "    else:\n",
        "        print(\"No blobs found in the bucket.\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "bucket_name = 'text-mining-source-dump'  # Replace with your bucket name\n",
        "#destination_folder = '/path/to/your/destination/folder'  # Replace with your desired folder path\n",
        "\n",
        "download_latest_blob_with_timestamp(bucket_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbEjmCNbwZIV"
      },
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "from google.oauth2 import service_account\n",
        "import os\n",
        "\n",
        "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
        "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
        "    SERVICE_ACCOUNT_FILE = 'amplified-brook-416922-6a39d3e05104.json'  # Replace if needed\n",
        "    credentials = service_account.Credentials.from_service_account_file(\n",
        "        SERVICE_ACCOUNT_FILE\n",
        "    )\n",
        "\n",
        "    storage_client = storage.Client(project='amplified-brook-416922', credentials=credentials)  # Replace project ID if needed\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(source_blob_name)\n",
        "\n",
        "    blob.download_to_filename(destination_file_name)\n",
        "\n",
        "    print(\n",
        "        f\"Blob {source_blob_name} downloaded to {destination_file_name}.\"\n",
        "    )\n",
        "\n",
        "# Example usage:\n",
        "bucket_name = \"text-mining-source-dump\"  # Replace with your bucket name\n",
        "source_blob_name = \"aggregated_data.csv\"  # Replace with the name of the file in the bucket\n",
        "destination_file_name = \"unlabeled_raw_data.csv\"  # Replace with the desired local filename\n",
        "\n",
        "download_blob(bucket_name, source_blob_name, destination_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdRktHwQitNa"
      },
      "outputs": [],
      "source": [
        "def get_review_vector(review, model):\n",
        "    review_vec = np.zeros(100)  # 100 is the vector size used in Word2Vec\n",
        "    count = 0\n",
        "    for word in review:\n",
        "        if word in model.wv.key_to_index:  # check if word is in the vocabulary\n",
        "            review_vec += model.wv[word]\n",
        "            count += 1\n",
        "    return review_vec / count if count > 0 else review_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1N0ewL47n2rt"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=3000)  # Use top 3000 words for features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LyZgvNLwcMH",
        "outputId": "880ce938-6974-4dd8-fdaf-5e40fcbe6a96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted labels saved to labeled_reviews1.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# ... (Your existing functions: preprocess_text, get_review_vector) ...\n",
        "\n",
        "# Load the saved SVM model\n",
        "model_save_name = 'classifier.joblib'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
        "svm_classifier = joblib.load(path)\n",
        "\n",
        "# Load the new dataset\n",
        "new_data = pd.read_csv('unlabeled_raw_data.csv')\n",
        "new_reviews = new_data['review_text']\n",
        "\n",
        "label_mapping = {1: 'positive', -1: 'negative', 0: 'neutral'}\n",
        "\n",
        "# Preprocess the new reviews\n",
        "new_reviews_tokenized = new_reviews.apply(preprocess_text)\n",
        "\n",
        "word2vec_model = Word2Vec(sentences=new_reviews_tokenized, vector_size=100, window=5, min_count=2, sg=1)\n",
        "\n",
        "\n",
        "# Convert to Word2Vec vectors\n",
        "new_X_word2vec = np.array([get_review_vector(review, word2vec_model) for review in new_reviews_tokenized])\n",
        "\n",
        "# Convert to TF-IDF vectors (assuming tfidf_vectorizer is already fitted)\n",
        "new_reviews_joined = [' '.join(review) for review in new_reviews_tokenized]\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(new_reviews_joined).toarray()\n",
        "X_combined = np.hstack((new_X_word2vec, X_tfidf))\n",
        "new_X_tfidf = tfidf_vectorizer.transform(new_reviews_joined).toarray()\n",
        "# Combine Word2Vec and TF-IDF features\n",
        "new_X_combined = np.hstack((new_X_word2vec, new_X_tfidf))\n",
        "\n",
        "# Predict the sentiment for each review using the loaded model\n",
        "new_predictions = svm_classifier.predict(new_X_combined)\n",
        "new_predictions_text = [label_mapping[pred] for pred in new_predictions]\n",
        "\n",
        "# Print or store the predictions as needed\n",
        "# for i, prediction in enumerate(new_predictions_text):\n",
        "#     print(f\"Review {i + 1}: {prediction}\")\n",
        "\n",
        "# Save the predictions in the original DataFrame\n",
        "new_data['predicted_sentiment'] = new_predictions_text\n",
        "\n",
        "# Save to a new CSV file\n",
        "new_data.to_csv('labeled_review1.csv', index=False)\n",
        "\n",
        "print(\"Predicted labels saved to labeled_reviews1.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import base64\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "#from .models import Review, VisualiData, ServiceModel\n",
        "\n",
        "\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "def generate_word_cloud(text, sentiment):\n",
        "    # Ensure stopwords are loaded\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Define stopwords to remove common and unhelpful words\n",
        "    custom_stopwords = {\"the\", \"and\", \"to\", \"in\", \"it\", \"is\", \"this\", \"that\", \"with\", \"for\", \"on\", \"as\", \"was\",\n",
        "                        \"are\", \"but\", \"be\", \"have\", \"at\", \"or\", \"from\", \"app\", \"bank\", \"service\", \"customer\", \"one\",\n",
        "                        \"like\", \"can\", \"get\", \"use\", \"using\", \"also\", \"would\", \"will\", \"make\", \"good\", \"bad\",\"app\", \"bank\", \"service\", \"customer\", \"one\", \"like\", \"can\", \"get\", \"use\", \"using\",\n",
        "                        \"also\", \"would\", \"will\", \"make\", \"still\", \"even\"}\n",
        "    stop_words.update(custom_stopwords)\n",
        "    text = preprocess_text(text)\n",
        "\n",
        "    # Tokenize text and filter out stopwords and short words\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    sentiment_words = []\n",
        "    # Initialize progress bar for word processing\n",
        "    print(\"Processing words for sentiment analysis:\")\n",
        "    for word in tqdm(words, desc=\"Analyzing\", unit=\"word\"):\n",
        "        if word not in stop_words and len(word) > 2:\n",
        "            # Analyze sentiment of each word using RoBERTa\n",
        "            inputs = tokenizer(word, return_tensors=\"pt\")\n",
        "            outputs = model(**inputs)\n",
        "            scores = torch.softmax(outputs.logits, dim=1).detach().numpy()[0]\n",
        "            positive_score, neutral_score, negative_score = scores[2], scores[1], scores[0]\n",
        "\n",
        "            # Filter words based on sentiment\n",
        "            if sentiment == 'positive' and positive_score > 0.4:\n",
        "                sentiment_words.append(word)\n",
        "            elif sentiment == 'negative' and negative_score > 0.3:\n",
        "                sentiment_words.append(word)\n",
        "\n",
        "    # Generate the word cloud\n",
        "    wordcloud = WordCloud(\n",
        "        width=400,\n",
        "        height=200,\n",
        "        background_color=\"white\",\n",
        "        colormap='Greens' if sentiment == 'positive' else 'Reds',  # Green for positive, red for negative\n",
        "        max_words=50\n",
        "    ).generate(\" \".join(sentiment_words))\n",
        "\n",
        "    # Convert the word cloud to a base64 image\n",
        "    buffer = BytesIO()\n",
        "    plt.figure(figsize=(4, 2))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.savefig(buffer, format=\"png\")\n",
        "    buffer.seek(0)\n",
        "    image_png = buffer.getvalue()\n",
        "    buffer.close()\n",
        "    return base64.b64encode(image_png).decode('utf-8')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "LcJvFoT2wlKo",
        "outputId": "d9a054e4-2bd2-45fc-8f2e-b9e185f762df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "attempted relative import with no known parent package",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-4032f82f4ab6>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVisualiData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mServiceModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('labeled_review1.csv')\n",
        "#service_list = read_services_from_gcs(\"labeled_review1.csv\")\n",
        "bank_name = [\"cibc\",\"td\",\"rbc\",\"scotiabank\",\"bmo\",\"nbc\"]\n",
        "service_list = [\"Credit\",\"Debit\",\"Fee\",\"Rates\",\"Mortgage\", \"Online banking\", \"Customer Service\", \"Interest Rates\", \"Insurance\",\n",
        "    \"Points\", \"Loan\", \"Interac\", \"Mobile banking\", \"Annual Fee\", \"Performance\",\n",
        "    \"Security\", \"No Fee\", \"Rewards\", \"Yield\", \"Features\", \"Quick Access\",\n",
        "    \"Mobile Deposit\", \"App Crash\"]\n",
        "\n",
        "visuali_data = analyze_service_sentiment(df, bank_name, service_name)\n",
        "# visuali_data.positive_reviews = summarize_reviews(visuali_data.positive_reviews)\n",
        "# visuali_data.negative_reviews = summarize_reviews(visuali_data.negative_reviews)\n",
        "\n",
        "\n",
        "\n",
        "# Refine text for positive and negative word clouds\n",
        "positive_text = \" \".join(visuali_data.positive_reviews)\n",
        "negative_text = \" \".join(visuali_data.negative_reviews)\n",
        "\n",
        "# Generate refined word clouds\n",
        "positive_wordcloud = generate_word_cloud(positive_text, sentiment='positive')\n",
        "negative_wordcloud = generate_word_cloud(negative_text, sentiment='negative')"
      ],
      "metadata": {
        "id": "R12w7uWK7VzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24MtDEtGwfh3"
      },
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "from google.oauth2 import service_account\n",
        "import os\n",
        "\n",
        "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
        "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
        "    SERVICE_ACCOUNT_FILE = 'amplified-brook-416922-6a39d3e05104.json'  # Replace if needed\n",
        "    credentials = service_account.Credentials.from_service_account_file(\n",
        "        SERVICE_ACCOUNT_FILE\n",
        "    )\n",
        "\n",
        "    storage_client = storage.Client(project='amplified-brook-416922', credentials=credentials)  # Replace project ID if needed\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(source_blob_name)\n",
        "\n",
        "    blob.download_to_filename(destination_file_name)\n",
        "\n",
        "    print(\n",
        "        f\"Blob {source_blob_name} downloaded to {destination_file_name}.\"\n",
        "    )\n",
        "\n",
        "# Example usage:\n",
        "bucket_name = \"text-mining-labeled-data\"  # Replace with your bucket name\n",
        "source_blob_name = \"labeled_reviews1\"  # Replace with the name of the file in the bucket\n",
        "destination_file_name = \"labeled_reviews.csv\"  # Replace with the desired local filename\n",
        "\n",
        "download_blob(bucket_name, source_blob_name, destination_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTyyOOWPwiiS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def concatenate_csv(file1, file2, output_file):\n",
        "    \"\"\"Concatenates two CSV files into a single CSV file.\n",
        "\n",
        "    Args:\n",
        "        file1 (str): Path to the first CSV file.\n",
        "        file2 (str): Path to the second CSV file.\n",
        "        output_file (str): Path to the output CSV file.\n",
        "    \"\"\"\n",
        "\n",
        "    # Read the CSV files into Pandas DataFrames\n",
        "    df1 = pd.read_csv(file1)\n",
        "    df2 = pd.read_csv(file2)\n",
        "\n",
        "    # Concatenate the DataFrames\n",
        "    concatenated_df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "    # Save the concatenated DataFrame to a new CSV file\n",
        "    concatenated_df.to_csv(output_file, index=False)\n",
        "\n",
        "    print(f\"CSV files concatenated and saved to {output_file}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "file1 = 'final_labeled_reviews.csv'\n",
        "file2 = 'labeled_review1.csv'\n",
        "output_file = 'concatenated_file.csv'\n",
        "\n",
        "concatenate_csv(file1, file2, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzOhDiiFwmH1"
      },
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "from google.oauth2 import service_account\n",
        "import os\n",
        "import csv\n",
        "\n",
        "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
        "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
        "    # The ID of your GCS bucket\n",
        "    #bucket_name = \"text-mining-source-dump\"\n",
        "    # The path to your file to upload\n",
        "    #source_file_name = \"C:\\Users\\Subhram Satyajeet\\OneDrive - University of Windsor\\Desktop\\Internship Project 2\\Review_Dataset_google_apple_net_banking\\Dataset\\bmo_google_before_2016.csv\"\n",
        "    # The ID of your GCS object\n",
        "    #destination_blob_name = \"bmo_google_before_2016\"\n",
        "\n",
        "    #Setting the service account credentials for authentication\n",
        "    SERVICE_ACCOUNT_FILE = 'amplified-brook-416922-6a39d3e05104.json' #key file name\n",
        "    credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE) #setting credentials using key file\n",
        "\n",
        "    storage_client = storage.Client(project='amplified-brook-416922' ,credentials = credentials)\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "    # Optional: set a generation-match precondition to avoid potential race conditions\n",
        "    # and data corruptions. The request to upload is aborted if the object's\n",
        "    # generation number does not match your precondition. For a destination\n",
        "    # object that does not yet exist, set the if_generation_match precondition to 0.\n",
        "    # If the destination object already exists in your bucket, set instead a\n",
        "    # generation-match precondition using its generation number.\n",
        "    generation_match_precondition = 0\n",
        "\n",
        "    blob.upload_from_filename(source_file_name, if_generation_match=generation_match_precondition)\n",
        "\n",
        "    print(\n",
        "        f\"File {source_file_name} uploaded to {destination_blob_name}.\"\n",
        "    )\n",
        "\n",
        "upload_blob('text-mining-labeled-data','concatenated_file.csv','final_labeled_reviews')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}