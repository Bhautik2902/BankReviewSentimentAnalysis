{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T23:48:44.741697Z",
     "start_time": "2024-10-06T23:48:42.470753Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import random \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T23:48:49.059432Z",
     "start_time": "2024-10-06T23:48:48.819489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ],
   "id": "f6fc582ccd6a3324",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9335e6d632a3192e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T23:50:50.742086Z",
     "start_time": "2024-10-06T23:50:50.282620Z"
    }
   },
   "cell_type": "code",
   "source": "df=pd.read_csv('canadian_banks_reddit_posts_2024_09_26-22_42_07.csv')",
   "id": "a0b6eb08a82cf136",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T00:02:32.475219Z",
     "start_time": "2024-10-07T00:02:31.096190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ALPHA=0.1\n",
    "BETA=0.1\n",
    "NUM_TOPICS=10\n",
    "sp=spacy.load('en_core_web_sm')\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ],
   "id": "74ab145ef1cf2fb7",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T02:30:24.743195Z",
     "start_time": "2024-10-07T02:30:16.491388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters, numbers, and punctuation\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = text.split()\n",
    "    \n",
    "    # Create a new list for lemmatized words\n",
    "    lemmatized_words = []\n",
    "    \n",
    "    # Remove stop words and lemmatize\n",
    "    for word in words: \n",
    "        if word not in stop_words and word.isalpha():\n",
    "            lemmatized_word = lemmatizer.lemmatize(word)\n",
    "            lemmatized_words.append(lemmatized_word)\n",
    "    \n",
    "    # Join lemmatized words back into a string\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply preprocessing to each row in the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    text = row['Selftext']\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    df.loc[index, 'cleaned_text'] = cleaned_text\n",
    "\n",
    "# Save the updated dataframe to CSV\n",
    "df.to_csv('cleaned_output_file_oct.csv', index=False)\n"
   ],
   "id": "30fbff2b30b088c9",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T02:32:05.567869Z",
     "start_time": "2024-10-07T02:31:26.424065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Initialize CountVectorizer (Bag of Words)\n",
    "vectorizer = CountVectorizer(max_df=0.9,min_df=2,stop_words='english')\n",
    "df=pd.read_csv('cleaned_output_file_oct.csv')\n",
    "# Remove rows where 'cleaned_text' is NaN\n",
    "df = df.dropna(subset=['cleaned_text'])\n",
    "#Apply CountVectorizer on cleaned_text column to get dtm (Document Term Matrix)\n",
    "dtm=vectorizer.fit_transform(df['cleaned_text'])\n",
    "# store the DTM in a new DataFrame\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "# Save the DTM to a CSV file\n",
    "dtm_df.to_csv('dtm_output_oct.csv', index=False)"
   ],
   "id": "2304607606d69fae",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T02:38:28.287673Z",
     "start_time": "2024-10-07T02:37:54.617021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Assuming dtm is your document-term matrix (from CountVectorizer)\n",
    "n_topics = 10  # Number of topics you expect\n",
    "\n",
    "# Initialize the LDA model\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "\n",
    "# Fit the LDA model to the document-term matrix\n",
    "lda_model.fit(dtm)"
   ],
   "id": "18f9c783c31cfa58",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T02:39:38.377178Z",
     "start_time": "2024-10-07T02:39:38.347802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {idx + 1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda_model, feature_names, 10)  # Display top 10 words per topic"
   ],
   "id": "cb311e0136803e0b",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T18:17:39.689847Z",
     "start_time": "2024-10-07T18:17:36.732851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "topic_distribution = lda_model.transform(dtm)\n",
    "\n",
    "# Example: Display topic distribution for the first review\n",
    "print(topic_distribution[0])  # Probabilities of each topic for the first review"
   ],
   "id": "4c0acdd3fedb1b71",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:37:19.736962Z",
     "start_time": "2024-10-07T19:37:19.166677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "topic_labels = {\n",
    "    0: 'Account Services and Transfers',  # Topic 1: Focuses on banking accounts, transfers, and customer service\n",
    "    1: 'Corporate Investments and Technology',  # Topic 2: Pertains to corporate stocks and technology growth\n",
    "    2: 'Credit Cards and Fees',  # Topic 3: Mentions credit cards, rates, and related fees\n",
    "    3: 'Market Trends and Investments',  # Topic 4: Covers buying, holding stocks, and market content (like Netflix)\n",
    "    4: 'Bank Fees and Charges',  # Topic 5: Discusses banking fees, payments, and savings\n",
    "    5: 'Credit Card Management',  # Topic 6: Relates to credit card usage and management issues\n",
    "    6: 'ETF and Investment Funds',  # Topic 7: Focuses on ETFs, investment stocks, and RBC investment\n",
    "    7: 'Canadian Banking Rates',  # Topic 8: Discusses Canadian market rates and shares\n",
    "    8: 'Stock Market Analysis',  # Topic 9: Covers stock prices and market revenue, including uranium\n",
    "    9: 'Mortgage Pricing and Services'  # Topic 10: Relates to mortgage pricing, shares, and market trends\n",
    "}\n",
    "# Get the dominant topic for each review\n",
    "df['dominant_topic'] = topic_distribution.argmax(axis=1)\n",
    "# Map the dominant topic to the topic labels\n",
    "df['classified_topic'] = df['dominant_topic'].map(topic_labels)\n",
    "# Save the updated DataFrame with the classified topics\n",
    "df.to_csv('updated_reviews_with_topics.csv', index=False)\n",
    "\n",
    "print(df[['cleaned_text', 'dominant_topic', 'classified_topic']].head())"
   ],
   "id": "92b3677c16d2fa11",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:41:29.167876Z",
     "start_time": "2024-10-07T19:41:28.651934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Count of reviews for each topic\n",
    "\n",
    "\n",
    "\n",
    "topic_df = pd.DataFrame(df)  # Convert topic_data into a DataFrame\n",
    "\n",
    "# Count of reviews for each topic name\n",
    "topic_counts = topic_df['classified_topic'].value_counts()\n",
    "\n",
    "# Plotting the count of reviews for each topic name\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=topic_counts.index, y=topic_counts.values, palette='viridis')\n",
    "plt.title('Count of Reviews per Topic')\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ],
   "id": "9a69a576c78d5035",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "5285d68e61616158",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
